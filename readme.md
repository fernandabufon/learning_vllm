# vLLM Open-Source Serving Experiments

This repository contains my personal **experiments and learnings** using **vLLM** to serve **open-source LLMs** through an **OpenAI-compatible API**, aiming for a faster and more organized inference workflow.

## ðŸŽ¯ Focus
- Serve open-source LLMs with **vLLM** (OpenAI-style API)
- Keep inference **fast, consistent, and easy to reuse**
- Document small **experiments, notes, and practical learnings**

## âœ… Whatâ€™s inside
- Minimal commands/scripts to start a vLLM server
- Quick client examples to query the API
- Notes about what worked, what didnâ€™t, and useful settings
